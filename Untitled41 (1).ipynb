{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EGzKbnT2KVve"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file = tf.keras.utils.get_file(origin='https://storage.googleapis.com/plantdata/PlantVillage.zip',\n",
        "                                   fname='PlantVillage.zip', extract=True)\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), 'PlantVillage')\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "validation_dir = os.path.join(data_dir, 'validation')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcIvQjwrUSMa",
        "outputId": "1e9e2061-2532-40eb-c50c-50fc7784e2c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/plantdata/PlantVillage.zip\n",
            "856839084/856839084 [==============================] - 47s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_width, img_height = 224, 224"
      ],
      "metadata": {
        "id": "dMn1Q7RpUSOy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=20,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=20,\n",
        "        class_mode='categorical')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aih7_0PQUSRT",
        "outputId": "a91445b2-c8e2-4936-dd54-dbb7e2518ccb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 43444 images belonging to 38 classes.\n",
            "Found 10861 images belonging to 38 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(512, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(512, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(38, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "KxzrBrtXUSU3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "j9CqZ2OpUilD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_history = cnn_model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
        "      epochs=5,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "      verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrV5pLW4Uion",
        "outputId": "b8f6ecad-6ba5-4557-ef6d-bea2da798a09"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2172/2172 [==============================] - 132s 58ms/step - loss: 1.8803 - accuracy: 0.4774 - val_loss: 0.8350 - val_accuracy: 0.7422\n",
            "Epoch 2/5\n",
            "2172/2172 [==============================] - 118s 54ms/step - loss: 0.6539 - accuracy: 0.8040 - val_loss: 0.5092 - val_accuracy: 0.8488\n",
            "Epoch 3/5\n",
            "2172/2172 [==============================] - 118s 54ms/step - loss: 0.4784 - accuracy: 0.8628 - val_loss: 0.4074 - val_accuracy: 0.8850\n",
            "Epoch 4/5\n",
            "2172/2172 [==============================] - 116s 53ms/step - loss: 0.4475 - accuracy: 0.8822 - val_loss: 0.5361 - val_accuracy: 0.8615\n",
            "Epoch 5/5\n",
            "2172/2172 [==============================] - 114s 52ms/step - loss: 0.4482 - accuracy: 0.8898 - val_loss: 0.6964 - val_accuracy: 0.8721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4LC541lUL3K",
        "outputId": "1a4d4a3f-5b0c-4229-b3ad-e1231636efdd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in vgg_base.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "id": "RaWkvXI3Uw3K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = Flatten()(cnn_model.output)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "output_layer = Dense(38, activation='softmax')(x)"
      ],
      "metadata": {
        "id": "MzeTheCpUw5I"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(cnn_model.input, output_layer)\n",
        "\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples/train_generator.batch_size,\n",
        "    epochs=5,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
        "    verbose=1)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwBaY2_DUw8A",
        "outputId": "b7ae9902-a4d1-418d-9d8d-155f141884c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "2172/2172 [==============================] - 117s 53ms/step - loss: 2.9008 - accuracy: 0.2712 - val_loss: 3.3497 - val_accuracy: 0.0986\n",
            "Epoch 2/5\n",
            "2172/2172 [==============================] - 114s 52ms/step - loss: 3.3491 - accuracy: 0.0993 - val_loss: 3.3458 - val_accuracy: 0.1015\n",
            "Epoch 3/5\n",
            "2172/2172 [==============================] - 114s 53ms/step - loss: 3.3475 - accuracy: 0.0989 - val_loss: 3.3455 - val_accuracy: 0.1015\n",
            "Epoch 4/5\n",
            "2172/2172 [==============================] - 114s 52ms/step - loss: 3.3472 - accuracy: 0.1002 - val_loss: 3.3479 - val_accuracy: 0.0986\n",
            "Epoch 5/5\n",
            "2172/2172 [==============================] - 114s 52ms/step - loss: 3.3469 - accuracy: 0.1004 - val_loss: 3.3474 - val_accuracy: 0.0986\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_input (InputLayer)   [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 26, 26, 128)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 256)       295168    \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 12, 12, 256)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 10, 10, 512)       1180160   \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 5, 5, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPoolin  (None, 1, 1, 512)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 38)                19494     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 38)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               19968     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 38)                19494     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4249996 (16.21 MB)\n",
            "Trainable params: 4249996 (16.21 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_grad_cam(model, img_path, target_size):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=target_size)\n",
        "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array /= 255.\n",
        "\n",
        "    # Find the last convolutional layer\n",
        "    last_conv_layer = None\n",
        "    for layer in model.layers[::-1]:\n",
        "        if isinstance(layer, Conv2D):\n",
        "            last_conv_layer = layer\n",
        "            break\n",
        "\n",
        "    if last_conv_layer is None:\n",
        "        raise ValueError(\"No convolutional layers found in the model.\")\n",
        "\n",
        "    grad_model = Model([model.inputs], [last_conv_layer.output, model.output])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_output, predictions = grad_model(img_array)\n",
        "        loss = predictions[:, np.argmax(predictions[0])]\n",
        "    grads = tape.gradient(loss, conv_output)[0]\n",
        "\n",
        "    weights = tf.reduce_mean(grads, axis=(0, 1))\n",
        "\n",
        "    cam = np.dot(conv_output[0], weights)\n",
        "\n",
        "    cam = cv2.resize(cam, (target_size[1], target_size[0]))\n",
        "    cam = np.maximum(cam, 0)\n",
        "    cam = cam / cam.max()\n",
        "\n",
        "    return cam\n",
        "\n",
        "\n",
        "\n",
        "# Choose an image for visualization\n",
        "img_path = '1.jpg'\n",
        "# Load the original image using matplotlib\n",
        "original_img = plt.imread(img_path)\n",
        "\n",
        "# Generate Grad-CAM heatmap\n",
        "heatmap = generate_grad_cam(model, img_path, (img_width, img_height))\n",
        "\n",
        "# Resize heatmap to match the original image size\n",
        "heatmap = cv2.resize(heatmap, (original_img.shape[1], original_img.shape[0]))\n",
        "\n",
        "# Convert heatmap to RGB\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "# Apply heatmap on the original image\n",
        "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "grad_cam_img = heatmap * 0.4 + original_img * 0.6  # Blend heatmap with the original image\n",
        "\n",
        "# Display the Grad-CAM image\n",
        "plt.imshow(grad_cam_img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "validation_generator.reset()  # Reset the generator\n",
        "pred = model.predict(validation_generator, steps=validation_generator.samples/validation_generator.batch_size, verbose=1)\n",
        "predicted_class_indices = np.argmax(pred, axis=1)\n",
        "\n",
        "# Get ground truth labels\n",
        "true_classes = validation_generator.classes\n",
        "class_labels = list(validation_generator.class_indices.keys())\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(true_classes, predicted_class_indices))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(true_classes, predicted_class_indices, target_names=class_labels, zero_division=1))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YavdREYeUw_c",
        "outputId": "6c8ce66e-84d7-4b27-bf4e-00b51a454bf6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-3af9f8d81c3a>:30: RuntimeWarning: invalid value encountered in divide\n",
            "  cam = cam / cam.max()\n",
            "<ipython-input-12-3af9f8d81c3a>:48: RuntimeWarning: invalid value encountered in cast\n",
            "  heatmap = np.uint8(255 * heatmap)\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "543/543 [==============================] - 20s 37ms/step\n",
            "Confusion Matrix:\n",
            "[[   0    0    0 ...  126    0    0]\n",
            " [   0    0    0 ...  125    0    0]\n",
            " [   0    0    0 ...   55    0    0]\n",
            " ...\n",
            " [   0    0    0 ... 1071    0    0]\n",
            " [   0    0    0 ...   74    0    0]\n",
            " [   0    0    0 ...  318    0    0]]\n",
            "Classification Report:\n",
            "                                                    precision    recall  f1-score   support\n",
            "\n",
            "                                Apple___Apple_scab       1.00      0.00      0.00       126\n",
            "                                 Apple___Black_rot       1.00      0.00      0.00       125\n",
            "                          Apple___Cedar_apple_rust       1.00      0.00      0.00        55\n",
            "                                   Apple___healthy       1.00      0.00      0.00       329\n",
            "                               Blueberry___healthy       1.00      0.00      0.00       300\n",
            "          Cherry_(including_sour)___Powdery_mildew       1.00      0.00      0.00       210\n",
            "                 Cherry_(including_sour)___healthy       1.00      0.00      0.00       170\n",
            "Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot       1.00      0.00      0.00       103\n",
            "                       Corn_(maize)___Common_rust_       1.00      0.00      0.00       239\n",
            "               Corn_(maize)___Northern_Leaf_Blight       1.00      0.00      0.00       197\n",
            "                            Corn_(maize)___healthy       1.00      0.00      0.00       233\n",
            "                                 Grape___Black_rot       1.00      0.00      0.00       236\n",
            "                      Grape___Esca_(Black_Measles)       1.00      0.00      0.00       276\n",
            "        Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      0.00      0.00       215\n",
            "                                   Grape___healthy       1.00      0.00      0.00        84\n",
            "          Orange___Haunglongbing_(Citrus_greening)       1.00      0.00      0.00      1102\n",
            "                            Peach___Bacterial_spot       0.00      0.00      0.00       459\n",
            "                                   Peach___healthy       1.00      0.00      0.00        72\n",
            "                     Pepper,_bell___Bacterial_spot       1.00      0.00      0.00       200\n",
            "                            Pepper,_bell___healthy       1.00      0.00      0.00       295\n",
            "                             Potato___Early_blight       1.00      0.00      0.00       200\n",
            "                              Potato___Late_blight       1.00      0.00      0.00       200\n",
            "                                  Potato___healthy       1.00      0.00      0.00        31\n",
            "                               Raspberry___healthy       1.00      0.00      0.00        74\n",
            "                                 Soybean___healthy       1.00      0.00      0.00      1018\n",
            "                           Squash___Powdery_mildew       1.00      0.00      0.00       367\n",
            "                          Strawberry___Leaf_scorch       1.00      0.00      0.00       222\n",
            "                              Strawberry___healthy       1.00      0.00      0.00        92\n",
            "                           Tomato___Bacterial_spot       1.00      0.00      0.00       425\n",
            "                             Tomato___Early_blight       1.00      0.00      0.00       200\n",
            "                              Tomato___Late_blight       1.00      0.00      0.00       382\n",
            "                                Tomato___Leaf_Mold       1.00      0.00      0.00       191\n",
            "                       Tomato___Septoria_leaf_spot       1.00      0.00      0.00       354\n",
            "     Tomato___Spider_mites Two-spotted_spider_mite       1.00      0.00      0.00       335\n",
            "                              Tomato___Target_Spot       1.00      0.00      0.00       281\n",
            "            Tomato___Tomato_Yellow_Leaf_Curl_Virus       0.10      1.00      0.18      1071\n",
            "                      Tomato___Tomato_mosaic_virus       1.00      0.00      0.00        74\n",
            "                                  Tomato___healthy       1.00      0.00      0.00       318\n",
            "\n",
            "                                          accuracy                           0.10     10861\n",
            "                                         macro avg       0.95      0.03      0.00     10861\n",
            "                                      weighted avg       0.87      0.10      0.02     10861\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# save the iris classification model as a pickle file\n",
        "model_pkl_file = \"saved_model.h5\"\n",
        "\n",
        "with open(model_pkl_file, 'wb') as file:\n",
        "    pickle.dump(model, file)\n"
      ],
      "metadata": {
        "id": "R1DphN76Qif6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsUhlJqfevIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}